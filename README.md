# Lyrics VerseÂ / Chorus Classification â€“ BERT **&** Longformer

This project provides two sibling training scripts that fineâ€‘tune Transformer encoders for deciding whether a lyric *segment* belongs to a **verse** (`0`) or **chorus** (`1`).

| Script                         | Backbone                       | MaxÂ Tokens | Recommended Batch |
| ------------------------------ | ------------------------------ | ---------- | ----------------- |
| `fine_tune_bert.py`            | `bertâ€‘baseâ€‘uncased`            | 512        | 8                 |
| `longformer_finetune_wandb.py` | `allenai/longformerâ€‘baseâ€‘4096` | 4â€¯096      | 2                 |

Both scripts **share the exact same data pipeline, CLI interface, logging, and naming conventions**; the only real difference is the underlying model.

---

## 1Â Â Setâ€‘up

```bash
# clone (or just `cd` if you already are in the repo)
git clone <repoâ€‘url> && cd ANLP-Project

# create an environment (conda or venv)
conda create -n lyrics python=3.10 -y
conda activate lyrics

# install requirements
pip install -r requirements.txt    # torch, transformers, wandb, datasets, tqdm, â€¦
```

---

## 2Â Â Data Format

`utils.preprocessing.get_data()` returns one `pandas.DataFrame` with columns:

| column    | dtype | description                         |
| --------- | ----- | ----------------------------------- |
| `text`    | str   | the segment (*target* to classify)  |
| `context` | str   | the rest of the song (can be blank) |
| `label`   | int   | 0Â =Â verse, 1Â =Â chorus               |

Each run performs an **80â€¯/â€¯20 random split** into train / validation â€“ **no external files required**.

---

## 3Â Â Common CLI Flags

| Flag              | Meaning                                             | Default                    |
| ----------------- | --------------------------------------------------- | -------------------------- |
| `--epochs`        | training epochs                                     | 1Â (BERT) /Â 10Â (Longformer) |
| `--batch_size`    | miniâ€‘batch size                                     | 8 /Â 2                      |
| `--lr`            | AdamW learningâ€‘rate                                 | 2eâ€‘5                       |
| `--output_dir`    | where checkpoints are written                       | `../checkpoints/<model>`   |
| `--no_context`    | ignore the `context` column (segmentâ€‘only baseline) | *disabled*                 |
| `--wandb_project` | WeightsÂ &Â Biases project name (omit to disable)     | `"bert"` / `"longformer"`  |
| `--run_name`      | custom run name in W\&B                             | autoâ€‘generated             |

Anything not exposed through the CLI (warmâ€‘up ratio, gradâ€‘clip, accumulation, etc.) is hardâ€‘coded in the scriptsâ€”feel free to tweak.

---

## 4Â Â Running BERT

```bash
python fine_tune_bert.py \
       --epochs 3 \
       --batch_size 8 \
       --lr 1e-5 \
       --wandb_project lyrics-parts \
       --run_name bert_ctx
# add --no_context for the segmentâ€‘only baseline
```

Generates a directory such as:

```
Results/fine_tuned_bert/lr1e-05_bs8_ep3_ctx_run-<id>/
â”œâ”€â”€ config.json
â”œâ”€â”€ pytorch_model.bin
â”œâ”€â”€ tokenizer.json
â””â”€â”€ â€¦
```

---

## 5Â Â Running Longformer

```bash
python longformer_finetune_wandb.py \
       --epochs 3 \
       --batch_size 2 \
       --lr 2e-5 \
       --wandb_project lyrics-parts \
       --run_name longformer_ctx
```

Produces:

```
Results/fine_tuned_longformer/lr2e-05_bs2_ep3_ctx_run-<id>/
â””â”€â”€ best_longformer.pt
```

---

## 6Â Â Monitoring with WeightsÂ &Â Biases

If `--wandb_project` is provided the scripts will automatically:

* log **train / val loss & accuracy** per epoch
* snapshot all hyperâ€‘parameters and the current Git SHA
* attach the best checkpoint as an *artifact*

```bash
wandb login                  # firstâ€‘time only
export WANDB_MODE=offline    # optional: run locally without uploading
```

---

## 7Â Â Inference Helper

After training you can score new lyrics with the shared utility:

```python
from utils.inference import get_predictions
preds_df = get_predictions(model, dataloader, device="cuda")
```

The resulting CSV contains `id`, `pred_label`, `prob_0`, `prob_1`.

---

## 8Â Â Reâ€‘using a Checkpoint

```python
from transformers import BertForSequenceClassification, BertTokenizer
model = BertForSequenceClassification.from_pretrained(
            "Results/fine_tuned_bert/â€¦")
tok   = BertTokenizer.from_pretrained("Results/fine_tuned_bert/â€¦")
```

Replace the class with `LongformerSegmentClassifier` if loading the Longformer weights.

---

Happy fineâ€‘tuning & have fun exploring your song dataÂ ðŸŽµ
